{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os  \n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nelsonfarrell/Documents/Northeastern/7180/projects/spectral_ratio\n"
     ]
    }
   ],
   "source": [
    "path = Path(os.getcwd())\n",
    "parent = path.parent\n",
    "parent = str(parent)\n",
    "path = str(path)\n",
    "print(path)\n",
    "sys.path.insert(1, path)\n",
    "sys.path.insert(1, parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.dataset_generator_class import ImageDatasetGenerator\n",
    "from models.CvT_model import CvT\n",
    "from angular_dist_loss.angular_dist_loss import CosineDistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Map Shape: torch.Size([1, 10, 10])\n",
      "Cosine Similarity Map (Sample): tensor([[[ 0.9220, -0.5231,  0.4831, -0.6378, -0.8759,  0.4608,  0.2581,\n",
      "          -0.8016, -0.5303,  0.9791],\n",
      "         [-0.3771,  0.7605, -0.5386,  0.4234,  0.9876, -0.7410, -0.0201,\n",
      "           0.0363,  0.6981, -0.7820],\n",
      "         [-0.4257, -0.1823,  0.6723, -0.3497, -0.4020,  0.0639, -0.0143,\n",
      "           0.1009,  0.9442,  0.8958],\n",
      "         [ 0.7189, -0.6682,  0.7644,  0.3612, -0.3193, -0.5670, -0.7822,\n",
      "          -0.4440,  0.8474,  0.1170],\n",
      "         [ 0.6589, -0.2912, -0.4562, -0.6379,  0.8324,  0.0086,  0.2314,\n",
      "           0.9306, -0.5040,  0.5117],\n",
      "         [ 0.6265,  0.7784, -0.9426, -0.1547, -0.1517, -0.6287,  0.7963,\n",
      "           0.3490, -0.7385,  0.3366],\n",
      "         [ 0.3273,  0.7947, -0.5527,  0.5817, -0.6719, -0.0039,  0.8875,\n",
      "           0.2409,  0.0701, -0.8399],\n",
      "         [-0.6304, -0.1355,  0.3797, -0.7333, -0.6063, -0.7245, -0.4952,\n",
      "          -0.2359,  0.9530, -0.0600],\n",
      "         [ 0.0460,  0.8652, -0.1980, -0.5718,  0.0474, -0.5036,  0.2520,\n",
      "          -0.7527, -0.7407,  0.7236],\n",
      "         [-0.2658,  0.0269,  0.5520, -0.3459,  0.1544,  0.7464, -0.0918,\n",
      "          -0.5051, -0.6988,  0.1544]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Create dummy image data with 3 color channels (RGB) and spatial dimensions 10x10\n",
    "height, width, channels = 10, 10, 3\n",
    "image1 = torch.randn(1, channels, height, width)  # Batch size = 1\n",
    "image2 = torch.randn(1, channels, height, width)  # Batch size = 1\n",
    "\n",
    "# Compute cosine similarity along the channel dimension (dim=1)\n",
    "cosine_similarity_map = F.cosine_similarity(image1, image2, dim=1)  # Shape: (1, height, width)\n",
    "\n",
    "# Inspect the result\n",
    "print(\"Cosine Similarity Map Shape:\", cosine_similarity_map.shape)\n",
    "print(\"Cosine Similarity Map (Sample):\", cosine_similarity_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms -- We can add augmentations here\n",
    "transform_images = transforms.Compose([ \n",
    "                        transforms.ToTensor()       \n",
    "                        #transforms.Normalize(mean = mean, std = std)\n",
    "                                    ])\n",
    "\n",
    "transform_guidance = transforms.Compose([ \n",
    "                        transforms.ToTensor()\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assertion Passed!!! We have the same number of images and ISD maps.\n",
      "Assertion Passed!!! We have the same number of images and ISD maps.\n",
      "Assertion Passed!!! We have the same number of images and ISD maps.\n"
     ]
    }
   ],
   "source": [
    "# Folder paths\n",
    "image_folder = \"/Users/nelsonfarrell/Documents/Northeastern/7180/projects/spectral_ratio/training_data/training_images_cropped\"\n",
    "isd_map_folder = \"/Users/nelsonfarrell/Documents/Northeastern/7180/projects/spectral_ratio/training_data/training_isds_cropped\"\n",
    "\n",
    "# Create dataset\n",
    "dataset = ImageDatasetGenerator(image_folder, \n",
    "                                isd_map_folder, \n",
    "                                split = None, \n",
    "                                val_size = 0.2, \n",
    "                                random_seed = 42, \n",
    "                                transform_images = transform_images, \n",
    "                                transform_guidance = transform_guidance)\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = ImageDatasetGenerator(image_folder, \n",
    "                                      isd_map_folder, \n",
    "                                      split = \"train\", \n",
    "                                      val_size = 0.2, \n",
    "                                      random_seed = 42, \n",
    "                                      transform_images = transform_images, \n",
    "                                      transform_guidance = transform_guidance)\n",
    "\n",
    "# Create dataset\n",
    "val_dataset = ImageDatasetGenerator(image_folder, \n",
    "                                    isd_map_folder, \n",
    "                                    split = \"val\", \n",
    "                                    val_size = 0.2, \n",
    "                                    random_seed = 42, \n",
    "                                    transform_images = transform_images, \n",
    "                                    transform_guidance = transform_guidance)\n",
    "\n",
    "# Create dataloader\n",
    "# Shuffle equals false for now for testing\n",
    "# drop last creates only full batches\n",
    "dataloader = DataLoader(val_dataset, batch_size = 4, shuffle = False, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering embedding: Patch Size: 7 -- Stride: 4 -- Embedding Dims 64\n",
      "Entering embedding: Patch Size: 3 -- Stride: 2 -- Embedding Dims 192\n",
      "Entering embedding: Patch Size: 3 -- Stride: 2 -- Embedding Dims 384\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  \n",
    "embed_size = 64\n",
    "num_class = 10\n",
    "model = CvT(embed_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/298 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 1.0288641452789307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/298 [00:01<06:34,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 0.8943011164665222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 2/298 [00:02<06:05,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 1.0367807149887085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 3/298 [00:03<05:45,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 0.9987214803695679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 4/298 [00:04<05:42,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 0.9484878182411194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 5/298 [00:05<05:35,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 1.0841166973114014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 6/298 [00:06<05:33,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 0.8827998042106628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 7/298 [00:08<05:27,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 0.9824261665344238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 8/298 [00:09<05:25,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 1.2715801000595093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 9/298 [00:10<05:22,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1: Shape x: torch.Size([4, 64, 55, 55])\n",
      "Stage 2: Shape x: torch.Size([4, 192, 27, 27])\n",
      "Stage 3: Shape x: torch.Size([4, 384, 13, 13])\n",
      "Loss: 0.9528747797012329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 10/298 [00:11<05:21,  1.12s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parameters\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 4\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = CosineDistanceMetric()  # Assuming regression; adjust for classification if needed\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Training Step\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        images, guidance_maps = batch\n",
    "        images, guidance_maps = images.to(device), guidance_maps.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, guidance_maps)\n",
    "        print(\"Loss:\", loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validation Step\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, guidance_maps = batch\n",
    "            images, guidance_maps = images.to(device), guidance_maps.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, guidance_maps)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "print(\"Training Complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b31d6a8ecfd6b48ec0cf9da67896692b418c4a39f70d4f1a3880fdc530a94b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
