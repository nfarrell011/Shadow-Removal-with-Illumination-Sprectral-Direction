{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Convolutional Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import Tensor\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIFFImageDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            folder_path (str): Path to the folder containing TIFF images.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        \"\"\"\n",
    "        self.folder_path = folder_path\n",
    "        self.image_paths = [\n",
    "            os.path.join(folder_path, fname)\n",
    "            for fname in os.listdir(folder_path)\n",
    "            if fname.lower().endswith(('.tif', '.tiff'))  # Include TIFF files\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the TIFF image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert to RGB\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Path to the folder containing TIFF images\n",
    "folder_path = \"/Users/nelsonfarrell/Documents/Northeastern/7180/projects/spectral_ratio/data/folder_3/processed_folder_3/high_quality\"\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 144x144\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create the dataset\n",
    "tiff_dataset = TIFFImageDataset(folder_path=folder_path, transform=transform)\n",
    "\n",
    "# Wrap the dataset in a DataLoader for batch processing\n",
    "dataloader = DataLoader(tiff_dataset, batch_size = 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: torch.Size([16, 3, 224, 224])\n",
      "Shape of one image: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, images in enumerate(dataloader):\n",
    "    print(f\"Batch Shape: {images.shape}\")  # (batch_size, channels, height, width)\n",
    "\n",
    "    # Access the first image in the batch and print its shape\n",
    "    single_image = images[0]  # Access the first image in the batch\n",
    "    print(f\"Shape of one image: {single_image.shape}\")  # (channels, height, width)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image_batch = single_image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Embeddings Shape: torch.Size([1, 3136, 128])\n"
     ]
    }
   ],
   "source": [
    "class MyPatchEmbedding(nn.Module):\n",
    "    \"\"\" \n",
    "    Creates a non-overlapping patch embedding\n",
    "\n",
    "    Dims in: 1, 3, 144, 144\n",
    "    Dims out: 1, 2304, 128\n",
    "        * 144 / 3 = 48   -- The number of windows that fit the in image.\n",
    "        * 48 * 48 = 2304 -- The number of positional embeddings.\n",
    "        * 128            -- The size of each embedding.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels = 3, patch_size = 4, stride = 4, emb_size = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        # \n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels = in_channels,\n",
    "            out_channels = emb_size,  # Embedding dims\n",
    "            kernel_size = patch_size,\n",
    "            stride = stride\n",
    "        )\n",
    "        \n",
    "        # Rearrange functions flattens the patches, but maintains the embedding.\n",
    "        self.rearrange = Rearrange('b c h w -> b (h w) c')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Applies conv2d and generates patch embeddings\n",
    "        \"\"\"\n",
    "        # Apply convolution to generate patch embeddings\n",
    "        patches = self.conv(x)\n",
    "\n",
    "        # Rearrange to sequence format\n",
    "        patches = self.rearrange(patches)\n",
    "        return patches\n",
    "\n",
    "\n",
    "# Initialize the patch embedding layer\n",
    "patch_embedding = MyPatchEmbedding(in_channels = 3, patch_size = 4, stride = 4, emb_size = 128)\n",
    "\n",
    "# Forward pass\n",
    "patches = patch_embedding(single_image_batch)\n",
    "print(\"Patch Embeddings Shape:\", patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(num_patches, embedding_size):\n",
    "    \"\"\" \n",
    "    Generates a positional encoding\n",
    "    \"\"\"\n",
    "    result = torch.ones(num_patches, embedding_size)\n",
    "    for i in range(num_patches):\n",
    "        for j in range(embedding_size):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / embedding_size))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / embedding_size)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" \n",
    "    Performs Multihead attention\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.att = torch.nn.MultiheadAttention(embed_dim = emb_dim,\n",
    "                                               num_heads = n_heads,\n",
    "                                               dropout = dropout)\n",
    "        # Fully connected linear layers\n",
    "        self.q = torch.nn.Linear(emb_dim, emb_dim)\n",
    "        self.k = torch.nn.Linear(emb_dim, emb_dim)\n",
    "        self.v = torch.nn.Linear(emb_dim, emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        attn_output, attn_output_weights = self.att(q, k, v)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    \"\"\" \n",
    "    Performs layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, fn):\n",
    "        super().__init__()\n",
    "\n",
    "        # Norm function\n",
    "        self.norm = nn.LayerNorm(emb_dim)\n",
    "\n",
    "        # Outer function ~ here Attention\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = PreNorm(emb_dim = 128, fn = Attention(emb_dim=128, n_heads=4, dropout=0.))\n",
    "norm(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    def __init__(self, emb_dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, emb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "ff = FeedForward(emb_dim = 128, hidden_dim = 256)\n",
    "ff(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Attention.__init__() got an unexpected keyword argument 'in_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 72\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape end: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 72\u001b[0m model \u001b[39m=\u001b[39m ViT()\n\u001b[1;32m     73\u001b[0m \u001b[39m#print(model)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39mprint\u001b[39m(single_image_batch\u001b[39m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[13], line 37\u001b[0m, in \u001b[0;36mViT.__init__\u001b[0;34m(self, channels_in, img_size, patch_size, emb_dim, n_layers, dropout, heads)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([])\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_layers):\n\u001b[1;32m     34\u001b[0m     transformer_block \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m         \u001b[39m# MHSA\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m         ResidualAdd(PreNorm(emb_dim, Attention(in_dim \u001b[39m=\u001b[39;49m emb_dim, num_heads \u001b[39m=\u001b[39;49m heads, kernel_size \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m))),\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m         \u001b[39m# Feed forward linear layer\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout \u001b[39m=\u001b[39m dropout))))\n\u001b[1;32m     41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mappend(transformer_block)\n",
      "\u001b[0;31mTypeError\u001b[0m: Attention.__init__() got an unexpected keyword argument 'in_dim'"
     ]
    }
   ],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 channels_in = 3, \n",
    "                 img_size = 224, \n",
    "                 patch_size = 16, \n",
    "                 emb_dim = 768,\n",
    "                 n_layers = 2, \n",
    "                 dropout = 0.1, \n",
    "                 heads = 12):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.channels = channels_in\n",
    "        self.height = img_size\n",
    "        self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers # The number transformer layers\n",
    "        self.dim_out = (img_size ** 2) * self.channels\n",
    "\n",
    "        # Get the patch embeddings - This uses a CNN\n",
    "        self.patch_embedding = MyPatchEmbedding(in_channels = channels_in,\n",
    "                                                patch_size = patch_size, \n",
    "                                                stride = 16,\n",
    "                                                emb_size = emb_dim)\n",
    "        # Get the number of patches\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Get the postional encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            transformer_block = nn.Sequential(\n",
    "\n",
    "                # MHSA\n",
    "                ResidualAdd(PreNorm(emb_dim, Attention(in_dim = emb_dim, num_heads = heads, kernel_size = 3))),\n",
    "\n",
    "                # Feed forward linear layer\n",
    "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
    "            self.layers.append(transformer_block)\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        # Get patch embedding vectors\n",
    "        x = self.patch_embedding(img)\n",
    "\n",
    "        # The number of patches\n",
    "        batch_size, num_patches, emb_dim = x.shape\n",
    "\n",
    "        # Add the positional encoding\n",
    "        x += self.pos_embedding[:, :(num_patches)]\n",
    "\n",
    "        # Transformer layers\n",
    "        for i in range(self.n_layers):\n",
    "            print(f\"Layer {i} -- Shape: {x.shape}\")\n",
    "            x = self.layers[i](x)\n",
    "        \n",
    "        # Rearrange back to image dims\n",
    "        x = rearrange(x, 'b (h w) (patch_c ph pw) -> b patch_c (h ph) (w pw)', \n",
    "                         h=14, w=14, patch_c=3, ph=16, pw=16)\n",
    "\n",
    "\n",
    "        # Final fully connected layer\n",
    "        print(f\"Shape end: {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = ViT()\n",
    "#print(model)\n",
    "print(single_image_batch.shape)\n",
    "x = model(single_image_batch)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b31d6a8ecfd6b48ec0cf9da67896692b418c4a39f70d4f1a3880fdc530a94b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
