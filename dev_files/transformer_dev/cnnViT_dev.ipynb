{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "from einops import rearrange, repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RearrangeLayer(nn.Module):\n",
    "    def __init__(self, pattern, **dims):\n",
    "        \"\"\"\n",
    "        Custom layer for einops.rearrange.\n",
    "        \n",
    "        Args:\n",
    "            pattern (str): Rearrangement pattern.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pattern = pattern\n",
    "        self.dims = dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        return rearrange(x, self.pattern, **self.dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positional Encoding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Sinusoidal Positional Encoding Module.\n",
    "\n",
    "        Args:\n",
    "            emb_size (int): The size of the embedding dimension.\n",
    "            max_len (int): The maximum length of the sequence.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create a positional encoding matrix of shape (max_len, emb_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # Shape: (max_len, 1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, emb_size, 2).float() * (-math.log(10000.0) / emb_size)\n",
    "        )  # Scaling factor for even indices\n",
    "\n",
    "        # Compute sinusoidal values for even indices and cosines for odd indices\n",
    "        pe = torch.zeros(max_len, emb_size)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "\n",
    "        # Add a batch dimension and register as a buffer (non-trainable parameter)\n",
    "        self.register_buffer(\"positional_encoding\", pe.unsqueeze(0))  # Shape: (1, max_len, emb_size)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Add positional encoding to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, emb_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Positional encoded tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.positional_encoding[:, :seq_len, :]\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Patch Embedding with Conv2d:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "    Uses a COnv2 layer with stride=patch size to create the patch embeddings\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of channels in the input image.\n",
    "        patch_size (int): The size of the patch to extract.\n",
    "        embed_size (int): The size of the embedding dimension.\n",
    "        img_size (int): SIze of the input iamge.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, patch_size: int, embed_size: int, img_size: int):\n",
    "        self.patch_size=patch_size\n",
    "        super().__init__()\n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, \n",
    "                      out_channels=embed_size, \n",
    "                      kernel_size=patch_size, \n",
    "                      stride=patch_size),\n",
    "            # Rearrange (batch, embedding_dimensions, height, width) to (batch, height*width, embeding_dims)\n",
    "            RearrangeLayer('b e h w -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "        # class token\n",
    "        #self.cls_token = nn.Parameter(torch.rand(1, 1, embed_size))\n",
    "\n",
    "        # positional encoding\n",
    "        self.pos_embed = PositionalEncoding(embed_size, (img_size // patch_size)**2)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Creates patch embedding of input iamge\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, in_channel, image_size, image_size).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Conv2d with positional embedding output of shape ().\n",
    "        \"\"\"\n",
    "        # get batch size\n",
    "        b, _, _, _, = x.shape\n",
    "\n",
    "        # forward pass through conv2d\n",
    "        x = self.embed(x)\n",
    "        print(f'Conv2d output shape: {x.shape}')\n",
    "\n",
    "        # add positional encoding to each vector\n",
    "        x = self.pos_embed(x)\n",
    "        print(f'Conv2d with positional embedding output shape: {x.shape}')\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Patch Embedding Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape: torch.Size([4, 196, 768])\n",
      "Conv2d with positional embedding output shape: torch.Size([4, 196, 768])\n",
      "Embeddings shape: torch.Size([4, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "model = PatchEmbedding(in_channels=3, patch_size=16, embed_size=768, img_size=224)\n",
    "model.eval()\n",
    "\n",
    "# Dummy input image (batch_size=2, channels=3, height=224, width=224)\n",
    "dummy_input = torch.randn(4, 3, 224, 224)\n",
    "embeddings = model(dummy_input)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)  # Expected: (batch_size, num_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class CNNFeatureEmbedder(nn.Module):\n",
    "    def __init__(self, cnn_backbone=None, embed_dim=768, position_embedding=True):\n",
    "        \"\"\"\n",
    "        CNN-based embedding module for hybrid architectures.\n",
    "\n",
    "        Args:\n",
    "            cnn_backbone (nn.Module): CNN model for feature extraction. Default: ResNet-50.\n",
    "            embed_dim (int): Target embedding dimension for the Transformer.\n",
    "            position_embedding (bool): Whether to add positional embeddings to the sequence.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN Backbone\n",
    "        if cnn_backbone is None:\n",
    "            cnn_backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "            self.feature_extractor = nn.Sequential(*list(cnn_backbone.children())[:-2])  # Up to final conv layer\n",
    "        else:\n",
    "            self.feature_extractor = cnn_backbone\n",
    "\n",
    "        # Patch embedding projection\n",
    "        self.patch_embed = nn.Linear(cnn_backbone.fc.in_features, embed_dim)\n",
    "\n",
    "        # Positional embedding (to be computed dynamically based on feature map size)\n",
    "        self.position_embedding = position_embedding\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the CNN-based embedding module.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input image tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Sequence of patch embeddings of shape (batch_size, num_patches, embed_dim).\n",
    "        \"\"\"\n",
    "        # Extract CNN feature maps\n",
    "        print(\"\\nCNN\")\n",
    "        features = self.feature_extractor(x)  # Shape: (batch_size, C, H, W)\n",
    "        print(f\"Features shape: {features.shape}\")\n",
    "\n",
    "        # Flatten spatial dimensions\n",
    "        batch_size, C, H, W = features.shape\n",
    "        print(f\"Feature maps: {H}x{W}\")\n",
    "        num_patches = H * W\n",
    "        print(f\"Number of patches: {num_patches}\")\n",
    "        features = features.permute(0, 2, 3, 1).reshape(batch_size, num_patches, C)  # Shape: (batch_size, num_patches, C)\n",
    "        print(f\"Flattened features shape: {features.shape}\")\n",
    "\n",
    "        # Project to Transformer embedding dimension\n",
    "        embeddings = self.patch_embed(features)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "        print(f\"Featues shape after linear projection to embed_dim={self.embed_dim}: {embeddings.shape}\")\n",
    "        # Add positional embedding\n",
    "        if self.position_embedding:\n",
    "            pos_embed = PositionalEncoding(self.embed_dim, max_len=num_patches)\n",
    "            embeddings = pos_embed(embeddings)  # Shape: (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Embedding Output Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN\n",
      "Features shape: torch.Size([4, 2048, 7, 7])\n",
      "Feature maps: 7x7\n",
      "Number of patches: 49\n",
      "Flattened features shape: torch.Size([4, 49, 2048])\n",
      "Featues shape after linear projection to embed_dim=768: torch.Size([4, 49, 768])\n",
      "Embeddings shape: torch.Size([4, 49, 768])\n",
      "Expected: (batch_size=2, num_patches=49, embed_dim=768)\n"
     ]
    }
   ],
   "source": [
    "model = CNNFeatureEmbedder(embed_dim=768)\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(4, 3, 224, 224)\n",
    "embeddings = model(dummy_input)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)  \n",
    "print(f\"Expected: (batch_size=2, num_patches=49, embed_dim=768)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4*embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*embed_dim, embed_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"\\nTransformerBlock\")\n",
    "        residual1 = x # first residual\n",
    "        print(f'residual1 shape: {residual1.shape}')\n",
    "\n",
    "        x = self.layer_norm(x) # first layer norm\n",
    "        print(f'self attention input shape: {x.shape}')\n",
    "        x, _ = self.mha(x, x, x) # MultiHeaded Attention\n",
    "        print(f'self attention output shape: {x.shape}')\n",
    "        x = self.dropout(x) + residual1 # First dropout layer w/ skip connection\n",
    "\n",
    "        residual2 = x\n",
    "        print(f'residual2 shape: {residual1.shape}')\n",
    "        x = self.layer_norm(x) # Second Layernorm\n",
    "        print(f'mlp input shape: {x.shape}')\n",
    "        x = self.feed_forward(x) # MLP with GeLU\n",
    "        print(f'mlp output shape: {x.shape}')\n",
    "        x = self.dropout(x) + residual2  # Second Dropout w/ skip connection  \n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size = 14\n",
      "Size = 28\n",
      "Size = 56\n",
      "Size = 112\n",
      "Size = 224\n",
      "Size = 448\n"
     ]
    }
   ],
   "source": [
    "def calc_output(input_size):\n",
    "    stride = 2\n",
    "    padding = 1\n",
    "    kernel_size = 4\n",
    "    output = (input_size-1) * stride - 2 * padding + kernel_size\n",
    "    print(f\"Size = {output}\")\n",
    "    return output\n",
    "\n",
    "input = 7\n",
    "while input <= 224:\n",
    "    input = calc_output(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, in_channels=768):\n",
    "        \"\"\"\n",
    "        Upsample ResNet feature map back to input size using transpose convolutions.\n",
    "\n",
    "        Output Size = (Input Size - 1) x (Stride - 2) x (Padding + Kernel Size)\n",
    "\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels from ResNet feature map.\n",
    "            target_size (tuple): Target spatial size (height, width) to upsample to.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Transpose convolution layers\n",
    "        self.upsample = nn.Sequential(\n",
    "            RearrangeLayer('b (h w) c -> b c h w', h=7, w=7), # reshape (batch, size, embed_dim) -> (batch, channel, height, width)\n",
    "            nn.ConvTranspose2d(in_channels, 512, kernel_size=4, stride=2, padding=1),  # 7 -> 14\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),          # 14 -> 28\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),          # 28 -> 56\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),           # 56 -> 112\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),             # 112 -> 224\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass to upsample feature map to input size.\n",
    "        Args:\n",
    "            x (Tensor): Input feature map from ResNet of shape (batch_size, in_channels, height, width).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Upsampled feature map of shape (batch_size, 3, target_size[0], target_size[1]).\n",
    "        \"\"\"\n",
    "        print(f\"\\nTransposeCNN input: {x.shape}\")\n",
    "        output =  self.upsample(x)\n",
    "        print(f\"TransposeCNN output: {output.shape}\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Decoder Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TransposeCNN input: torch.Size([4, 49, 768])\n",
      "TransposeCNN output: torch.Size([4, 3, 224, 224])\n",
      "Feature map shape: torch.Size([4, 49, 768])\n",
      "Decoded image shape: torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, 49, 768) \n",
    "\n",
    "decoded = SimpleDecoder(in_channels=768)\n",
    "decoded_image = decoded(input)  \n",
    "\n",
    "print(\"Feature map shape:\", input.shape)\n",
    "print(\"Decoded image shape:\", decoded_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, num_layers, img_size, embed_dim, patch_size, num_head, cnn_embedding=True):\n",
    "    super().__init__()\n",
    "    # patch embedding\n",
    "\n",
    "    if cnn_embedding == True:\n",
    "      #Input embedding\n",
    "      self.patch_emb = CNNFeatureEmbedder(embed_dim=embed_dim)\n",
    "      #Output decoder\n",
    "      self.output_layer = SimpleDecoder(in_channels=embed_dim)\n",
    "    else:\n",
    "      # Input embedding\n",
    "      self.patch_emb = PatchEmbedding(in_channels=3, patch_size=patch_size, img_size=img_size, embed_size=embed_dim)\n",
    "      # Output reshaping\n",
    "      self.output_layer = nn.Sequential(RearrangeLayer('b (h w) (patch_c ph pw) -> b patch_c (h ph) (w pw)', \n",
    "                         h=14, w=14, patch_c=3, ph=16, pw=16))\n",
    "\n",
    "    # Transformer layers\n",
    "    self.trans_encoder = nn.Sequential(*[TransformerEncoderBlock(embed_dim, num_head) for layer in range(num_layers)])\n",
    "\n",
    "    \n",
    "\n",
    "  def forward(self, x): # input: [b, c, h, w]\n",
    "    print(f\"input shape: {x.shape}\")\n",
    "    # Get patch embeddings w/ positional encoding\n",
    "    patch_embeddings = self.patch_emb(x) # patch_embeddings: [b, (h*w), e]\n",
    "    print(f'patch_embeddings shape: {patch_embeddings.shape}')\n",
    "\n",
    "    # Transformer encoding layers\n",
    "    x = self.trans_encoder(patch_embeddings) # x: [b, (h*w), e]\n",
    "    print(f'transformer output shape: {x.shape}')\n",
    "    \n",
    "    # reshape embedding into image\n",
    "    output_img = self.output_layer(x) #output_img: [b, c, h, w]\n",
    "\n",
    "    return output_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_layers = 2\n",
    "embed_dim = 768\n",
    "num_head = 12\n",
    "patch_size=16\n",
    "img_size=224\n",
    "model = VisionTransformer( num_layers=num_layers,\n",
    "                            img_size=img_size,\n",
    "                            embed_dim=embed_dim,\n",
    "                            patch_size=patch_size,\n",
    "                            num_head=num_head\n",
    "                            ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_emb): CNNFeatureEmbedder(\n",
       "    (feature_extractor): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (patch_embed): Linear(in_features=2048, out_features=768, bias=True)\n",
       "  )\n",
       "  (output_layer): SimpleDecoder(\n",
       "    (upsample): Sequential(\n",
       "      (0): RearrangeLayer()\n",
       "      (1): ConvTranspose2d(768, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ReLU()\n",
       "      (4): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (9): ReLU()\n",
       "      (10): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ReLU()\n",
       "      (13): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (trans_encoder): Sequential(\n",
       "    (0): TransformerEncoderBlock(\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerEncoderBlock(\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Input shape: torch.Size([4, 3, 224, 224])\n",
      "input shape: torch.Size([4, 3, 224, 224])\n",
      "\n",
      "CNN\n",
      "Features shape: torch.Size([4, 2048, 7, 7])\n",
      "Feature maps: 7x7\n",
      "Number of patches: 49\n",
      "Flattened features shape: torch.Size([4, 49, 2048])\n",
      "Featues shape after linear projection to embed_dim=768: torch.Size([4, 49, 768])\n",
      "patch_embeddings shape: torch.Size([4, 49, 768])\n",
      "\n",
      "TransformerBlock\n",
      "residual1 shape: torch.Size([4, 49, 768])\n",
      "self attention input shape: torch.Size([4, 49, 768])\n",
      "self attention output shape: torch.Size([4, 49, 768])\n",
      "residual2 shape: torch.Size([4, 49, 768])\n",
      "mlp input shape: torch.Size([4, 49, 768])\n",
      "mlp output shape: torch.Size([4, 49, 768])\n",
      "\n",
      "TransformerBlock\n",
      "residual1 shape: torch.Size([4, 49, 768])\n",
      "self attention input shape: torch.Size([4, 49, 768])\n",
      "self attention output shape: torch.Size([4, 49, 768])\n",
      "residual2 shape: torch.Size([4, 49, 768])\n",
      "mlp input shape: torch.Size([4, 49, 768])\n",
      "mlp output shape: torch.Size([4, 49, 768])\n",
      "transformer output shape: torch.Size([4, 49, 768])\n",
      "\n",
      "TransposeCNN input: torch.Size([4, 49, 768])\n",
      "TransposeCNN output: torch.Size([4, 3, 224, 224])\n",
      "\n",
      "Model  sOutput shape: torch.Size([4, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Test input\n",
    "batch_size = 4\n",
    "test_input = torch.rand(batch_size, 3, img_size, img_size).to(device)\n",
    "\n",
    "# Forward pass\n",
    "try:\n",
    "    print(\"Model Input shape:\", test_input.shape)\n",
    "    output = model(test_input)\n",
    "    print(\"\\nModel  sOutput shape:\", output.shape)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the forward pass: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class CNNFeatureEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-based embedding module for hybrid architectures.\n",
    "\n",
    "    Args:\n",
    "        cnn_backbone (nn.Module): CNN model for feature extraction. Default: ResNet-50.\n",
    "        embed_dim (int): Target embedding dimension for the Transformer.\n",
    "        position_embedding (bool): Whether to add positional embeddings to the sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained=True, embed_dim=768, position_embedding=True):\n",
    "        super().__init__()\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.info(f\"Initializing CNNFeatureEmbedder with embed_dim={embed_dim}, position_embedding={position_embedding}\")\n",
    "        \n",
    "        if pretrained:\n",
    "            cnn_backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "            self.feature_extractor = nn.Sequential(*list(cnn_backbone.children())[:-2])\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            cnn_backbone = resnet50()\n",
    "            self.feature_extractor = nn.Sequential(*list(cnn_backbone.children())[:-2])\n",
    "\n",
    "        self.patch_embed = nn.Linear(cnn_backbone.fc.in_features, embed_dim)\n",
    "        self.position_embedding = position_embedding\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the CNN-based embedding module.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input image tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Sequence of patch embeddings of shape (batch_size, num_patches, embed_dim).\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Input image tensor shape: {x.shape}\")\n",
    "        features = self.feature_extractor(x)\n",
    "        self.logger.debug(f\"Features shape: {features.shape}\")\n",
    "        \n",
    "        batch_size, C, H, W = features.shape\n",
    "        self.logger.debug(f\"Feature map dims: {H}x{W}\")\n",
    "        num_patches = H * W\n",
    "        self.logger.debug(f\"Number of patches: {num_patches}\")\n",
    "        \n",
    "        features = features.permute(0, 2, 3, 1).reshape(batch_size, num_patches, C)\n",
    "        self.logger.debug(f\"Flattened features shape: {features.shape}\")\n",
    "\n",
    "        embeddings = self.patch_embed(features)\n",
    "        self.logger.debug(f\"Embedding shape after projection: {embeddings.shape}\")\n",
    "        \n",
    "        if self.position_embedding:\n",
    "            pos_embed = PositionalEncoding(self.embed_dim, max_len=num_patches)\n",
    "            embeddings = pos_embed(embeddings)\n",
    "            \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([4, 49, 768])\n",
      "Expected: (batch_size=2, num_patches=49, embed_dim=768)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "model = CNNFeatureEmbedder(pretrained=False, position_embedding=False, embed_dim=3)\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(4, 3, 224, 224)\n",
    "embeddings = model(dummy_input)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)  \n",
    "print(f\"Expected: (batch_size=2, num_patches=49, embed_dim=768)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS5330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
