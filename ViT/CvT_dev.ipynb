{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Convolutional Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import matplotlib.pyplot as plt\n",
    "from random import random\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import Tensor\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDatasetGenerator(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            folder_path (str): Path to the images. Must be tiffs currently.\n",
    "            transform (callable): A function/transform to apply to the images. Optional, default = None\n",
    "        \"\"\"\n",
    "        self.folder_path = folder_path\n",
    "        self.image_paths = [\n",
    "            os.path.join(folder_path, file)\n",
    "            for file in os.listdir(folder_path)\n",
    "            if file.lower().endswith(('.tif', '.tiff')) \n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of image paths.\n",
    "        For capability with Torch DataLoader. \n",
    "        \"\"\"\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" \n",
    "        Get the image item.\n",
    "        For capability with Torch Dataloader\n",
    "        \"\"\"\n",
    "        # Load the TIFF image\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Perform tranforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Image folder path\n",
    "folder_path = \"/Users/nelsonfarrell/Documents/Northeastern/7180/projects/spectral_ratio/data/folder_3/processed_folder_3/high_quality\"\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 144x144\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "tiff_dataset = ImageDatasetGenerator(folder_path=folder_path, transform=transform)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(tiff_dataset, batch_size = 16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 - Image Tensor Shape: torch.Size([16, 3, 224, 224])\n",
      "Batch 2 - Image Tensor Shape: torch.Size([16, 3, 224, 224])\n",
      "Batch 3 - Image Tensor Shape: torch.Size([16, 3, 224, 224])\n",
      "Batch 4 - Image Tensor Shape: torch.Size([16, 3, 224, 224])\n",
      "Batch 5 - Image Tensor Shape: torch.Size([16, 3, 224, 224])\n",
      "Batch 6 - Image Tensor Shape: torch.Size([16, 3, 224, 224])\n",
      "Batch 7 - Image Tensor Shape: torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for batch_idx, images in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1} - Image Tensor Shape: {images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: torch.Size([16, 3, 224, 224])\n",
      "Shape of one image: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, images in enumerate(dataloader):\n",
    "    print(f\"Batch Shape: {images.shape}\")\n",
    "    single_batch = images\n",
    "    \n",
    "    # Access one item in the batch\n",
    "    single_image = images[0] \n",
    "    print(f\"Shape of one image: {single_image.shape}\") \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch size 1\n",
    "single_image_batch = single_image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_image_batch.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### CvT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVTEmbedding(nn.Module):\n",
    "  \"\"\" \n",
    "  Convolutional Embedding Class\n",
    "  \"\"\"\n",
    "  def __init__(self, in_ch, embed_dim, patch_size, stride):\n",
    "    \"\"\" \n",
    "    Generates an embedding with conv2 and down sampling\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    print(f\"Entering embedding: Patch Size: {patch_size} -- Stride: {stride} -- Embedding Dims {embed_dim}\")\n",
    "    \n",
    "\t# Generate embedding with Conv2d and Norm\n",
    "    self.embed = nn.Sequential(nn.Conv2d(in_ch, embed_dim, kernel_size = patch_size, stride = stride))\n",
    "    self.norm = nn.LayerNorm(embed_dim)\n",
    "\t\n",
    "  def forward(self, x):\n",
    "    \"\"\" \n",
    "\tGenerates the embedding.\n",
    "  \t\"\"\"\n",
    "    print(f\"Shape X IN: {x.shape}\")\n",
    "    x = self.embed(x)\n",
    "    \n",
    "\t# Rearrange to batch_size, positional_embeddings, channels\n",
    "    x = rearrange(x, 'b c h w -> b (h w) c') # i.e. x: B T(h w) C\n",
    "    x = self.norm(x)\n",
    "    print(f\"Shape X OUT: {x.shape}\")\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Convolutional Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  \"\"\" \n",
    "  This class will replaces the linear projection of Q, K, V with conv2d\n",
    "  \"\"\"\n",
    "  def __init__(self, in_dim, num_heads, kernel_size = 3, with_cls_token = False):\n",
    "    super().__init__()\n",
    "    \"\"\" \n",
    "    Initializer for MHA\n",
    "\n",
    "    Parameters:\n",
    "      * in_dim: (int)           - Embedding size\n",
    "      * num_heads: (int)        - The number of attention heads\n",
    "      * kernel_size: (int)      - The kernel size in the conv2d\n",
    "      * with_cls_token: (bool)  - Used to perform classification\n",
    "    \"\"\"\n",
    "\n",
    "    # Padding to prevent downsampling\n",
    "    padding = (kernel_size - 1)//2\n",
    "\n",
    "    # The number of attention heads\n",
    "    self.num_heads = num_heads\n",
    "\n",
    "    # Used for classification; we will not use this, but could useful later\n",
    "    self.with_cls_token = with_cls_token\n",
    "\n",
    "    # Projection operations; Conv2d, BatchNorm, and Rearrange\n",
    "    self.conv = nn.Sequential(\n",
    "        nn.Conv2d(in_dim, in_dim, kernel_size = kernel_size, padding = padding, stride = 1),\n",
    "        nn.BatchNorm2d(in_dim),\n",
    "        Rearrange('b c h w -> b (h w) c') # FROM (batch_size, height, width, channels) TO (batch_size, positional_embeddings, embedding_size)\n",
    "    )\n",
    "\n",
    "    # Set attention drop out rate\n",
    "    self.att_drop = nn.Dropout(0.1)\n",
    "\n",
    "  def forward_conv(self, x):\n",
    "    \"\"\" \n",
    "    Performs convolution on x three times\n",
    "\n",
    "    Parameters:\n",
    "        * x: (Tensor) - batch_size, positional_embeddings, embedding_size\n",
    "    \n",
    "    Returns:\n",
    "        * Q, K, V\n",
    "    \"\"\"\n",
    "    # Used for classification\n",
    "    B, hw, C = x.shape\n",
    "    if self.with_cls_token:\n",
    "      cls_token, x = torch.split(x, [1, hw-1], 1)\n",
    "    \n",
    "    # Height and width equal the number of tokens; num positional embeddings\n",
    "    H = W = int(x.shape[1]**0.5)\n",
    "\n",
    "    # Rearrange x\n",
    "    # FROM (batch_size, positional_embeddings, embedding_size) TO (batch_size, height, width, channels)\n",
    "    # Brings back to image dimensions\n",
    "    x = rearrange(x, 'b (h w) c -> b c h w', h = H, w = W)\n",
    "    \n",
    "    # Convolution for Q, K, V\n",
    "    q = self.conv(x)\n",
    "    k = self.conv(x)\n",
    "    v = self.conv(x)\n",
    "\n",
    "    if self.with_cls_token:\n",
    "            q = torch.cat((cls_token, q), dim=1)\n",
    "            k = torch.cat((cls_token, k), dim=1)\n",
    "            v = torch.cat((cls_token, v), dim=1)\n",
    "\n",
    "\n",
    "    # Return Q, K, V\n",
    "    return q, k, v\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" \n",
    "    Forward pass\n",
    "\n",
    "    Parameters:\n",
    "        * x: (Tensor) - (batch_size, positional_embeddings, embedding_size) or (b, t, c), where:\n",
    "                b: batch size\n",
    "                t: Number of positional embeddings or sequence length\n",
    "                c: (d * H) - Total embedding size (split into d for each head and H heads)\n",
    "                H: Number of heads\n",
    "                d: c/H - Dims of each head's embedding\n",
    "        Note: c % H = 0 MUST BE TRUE\n",
    "    \"\"\"\n",
    "    # Performs the convolutions on x\n",
    "    q, k, v = self.forward_conv(x)\n",
    "    #print(f\"Shape of q k v: {q.shape}\")\n",
    "\n",
    "    # Rearrange the output from the convolutions for attention computation\n",
    "    # FROM (batch_size, positional_embeddings, (size of the head embedding, number of heads))\n",
    "    # TO (batch_size, number of heads, positional_embeddings, head embedding)\n",
    "    # b: batch_size\n",
    "    # t: positional embeddings\n",
    "    # d: head embedding size\n",
    "    # H: number of heads\n",
    "    q = rearrange(x, 'b t (d H) -> b H t d', H = self.num_heads)\n",
    "    k = rearrange(x, 'b t (d H) -> b H t d', H = self.num_heads)\n",
    "    v = rearrange(x, 'b t (d H) -> b H t d', H = self.num_heads)\n",
    "\n",
    "    # Compute attention\n",
    "    # Q: (b, H, t, d)\n",
    "    # K^T: (b, H, d, T) -- Transposed along last 2 dimensions\n",
    "    # Allows matrix multiplication\n",
    "    # OUT: att_score: (b, H, t , t)\n",
    "    # Each attention head computes scores for all the positional embeddings; closer vectors get higher scores\n",
    "    att_score = q@k.transpose(2, 3) / self.num_heads**0.5 # This division stabilizes the gradient\n",
    "\n",
    "    # Perform soft along the last dimension\n",
    "    att_score = F.softmax(att_score, dim = -1)\n",
    "\n",
    "    # Apply dropout\n",
    "    att_score = self.att_drop(att_score)\n",
    "\n",
    "    # Compute attention scores\n",
    "    x = att_score@v\n",
    "    x = rearrange(x, 'b H t d -> b t (H d)')\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  \"\"\" \n",
    "  Create an MLP\n",
    "  \"\"\"\n",
    "  def __init__(self, dim):\n",
    "    super().__init__()\n",
    "    self.ff = nn.Sequential(\n",
    "                            nn.Linear(dim, 4*dim),\n",
    "                            nn.GELU(),\n",
    "                            nn.Dropout(0.1),\n",
    "                            nn.Linear(4 * dim, dim),\n",
    "                            nn.Dropout(0.1)\n",
    "                          )\n",
    "\n",
    "  def forward(self, x):\n",
    "     return self.ff(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  \"\"\" \n",
    "  Attention Block Class\n",
    "\n",
    "  This performs the entire attention pipe\n",
    "  \"\"\"\n",
    "  def __init__(self, embed_dim, num_heads, with_cls_token):\n",
    "    super().__init__()\n",
    "\n",
    "    # MHA\n",
    "    self.mhsa = MultiHeadAttention(embed_dim, num_heads, with_cls_token=with_cls_token)\n",
    "\n",
    "    # MLP\n",
    "    self.ff = MLP(embed_dim)\n",
    "\n",
    "    # Norms and regs\n",
    "    self.norm1 = nn.LayerNorm(embed_dim)\n",
    "    self.norm2 = nn.LayerNorm(embed_dim)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass \n",
    "    \"\"\"\n",
    "    # Performs MHA\n",
    "    x = x + self.dropout(self.mhsa(self.norm1(x)))\n",
    "\n",
    "    # Performs MLP\n",
    "    x = x + self.dropout(self.ff(self.norm2(x)))\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Vision Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "  \"\"\" \n",
    "  Vision Transformer class.\n",
    "\n",
    "  Generates embedding with a convolution layer (down samples) and performs MHA.\n",
    "\n",
    "  Params:\n",
    "    * depth: (int)      -- Controls the number of MHA blocks\n",
    "    * stride: (int)     -- Controls image down sampling for the embedding.\n",
    "    * patch_size: (int) -- Kernel size for the embedding conv\n",
    "    * in_ch: (int)      -- Number of channels in image before embedding.\n",
    "    * embed_dim: (int)  -- Number of dimensions in the embedding.\n",
    "    * num_heads: (int)  -- The nu\n",
    "  \"\"\"\n",
    "  def __init__(self, depth:int, embed_dim:int, num_heads:int, patch_size:int, stride:int, in_ch:int = 3, cls_token = False):\n",
    "    super().__init__()\n",
    "\n",
    "    # Embedding stride -- This downsamples the image\n",
    "    self.stride = stride\n",
    "\n",
    "    # For later use, classification\n",
    "    self.cls_token = cls_token\n",
    "    \n",
    "    # Generates the embedding for the transformer\n",
    "    self.embedding = CVTEmbedding(in_ch, embed_dim, patch_size, stride)\n",
    "\n",
    "    # Unpacks the list of Blocks controled by depth param. The is the number of attention blocks in the ViT layer.\n",
    "    self.layers = nn.Sequential(*[Block(embed_dim, num_heads, cls_token) for _ in range(depth)])\n",
    "\n",
    "    # Used for classification -- Not used here! \n",
    "    if self.cls_token:\n",
    "       self.cls_token_embed = nn.Parameter(torch.randn(1, 1, 384))\n",
    "\n",
    "  def forward(self, x, ch_out =False):\n",
    "    \"\"\" \n",
    "    Forward pass\n",
    "    \"\"\"\n",
    "    # Get the dims of x for rearranging \n",
    "    B, C, H, W = x.shape\n",
    "\n",
    "    # Get embedding \n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # Used for classification\n",
    "    if self.cls_token:\n",
    "      cls_token = repeat(self.cls_token_embed, ' () s e -> b s e', b=B)\n",
    "      x = torch.cat([cls_token, x], dim=1)\n",
    "\n",
    "    # Execute attention layers.\n",
    "    x = self.layers(x)\n",
    "\n",
    "    # Reshape to image shape\n",
    "    if not ch_out:\n",
    "       x = rearrange(x, 'b (h w) c -> b c h w', h = (H -1)//self.stride, w = (W-1)//self.stride)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Decoder Layers\n",
    "\n",
    "    This will bring the image back to original dims\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Stage 3 to Stage 2\n",
    "        self.decode_stage3 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(384, 192, kernel_size = 4, stride = 2, padding = 1, output_padding = 1),  # 13x13 -> 27x27\n",
    "                nn.BatchNorm2d(192),\n",
    "                nn.ReLU())\n",
    "\n",
    "        # Stage 2 to Stage 1\n",
    "        self.decode_stage2 = nn.Sequential(\n",
    "                nn.ConvTranspose2d(192, 64, kernel_size = 4, stride = 2, padding = 1, output_padding = 1), # 27x27 -> 55x55\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU())\n",
    "\n",
    "        # Stage 1 to Original Dims\n",
    "        self.decode_stage1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size = 9, stride = 4, padding = 1, output_padding = 1)) # 55x55 -> 224x224\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "        # Stage 3 -> Stage 2\n",
    "        x = self.decode_stage3(x)\n",
    "        print(f\"After Stage 3 -> 2: {x.shape}\")  \n",
    "\n",
    "        # Stage 2 -> Stage 1\n",
    "        x = self.decode_stage2(x)\n",
    "        print(f\"After Stage 2 -> 1: {x.shape}\")  \n",
    "\n",
    "        # Stage 1 -> Original Image\n",
    "        x = self.decode_stage1(x)\n",
    "        print(f\"After Stage 1 -> Original: {x.shape}\")  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvT(nn.Module):\n",
    "  \"\"\" \n",
    "  Convolutional Vision Transformer\n",
    "\n",
    "  Executes a 3-stage vision transformer with a decoder to get back to image dims\n",
    "  \"\"\"\n",
    "  def __init__(self, embed_dim, cls_token:bool = False, num_class:int = 10):\n",
    "    super().__init__()\n",
    "    self.cls_token = cls_token\n",
    "\n",
    "    self.stage1 = VisionTransformer(depth = 1,\n",
    "                                     embed_dim = 64,\n",
    "                                     num_heads = 1,\n",
    "                                     patch_size = 7,\n",
    "                                     stride = 4,\n",
    "                                     )\n",
    "    \n",
    "    self.stage2 = VisionTransformer(depth = 2,\n",
    "                                     embed_dim = 192,\n",
    "                                     num_heads = 3,\n",
    "                                     patch_size = 3,\n",
    "                                     stride = 2,\n",
    "                                     in_ch = 64\n",
    "                                     )\n",
    "    \n",
    "    self.stage3 = VisionTransformer(depth = 10,\n",
    "                                     embed_dim = 384,\n",
    "                                     num_heads = 6,\n",
    "                                     patch_size = 3,\n",
    "                                     stride = 2,\n",
    "                                     in_ch = 192,\n",
    "                                     cls_token = self.cls_token\n",
    "                                     )\n",
    "    \n",
    "\t# For classifiction - Not used! \n",
    "    self.ff = nn.Sequential(\n",
    "        nn.Linear(6 * embed_dim, embed_dim),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(embed_dim, num_class)\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    \"\"\" \n",
    "    Forward Pass\n",
    "    \"\"\"\n",
    "    # Stage 1\n",
    "    x = self.stage1(x)\n",
    "    print(f\"Stage 1: Shape x: {x.shape}\")\n",
    "\n",
    "    # Stage 2\n",
    "    x = self.stage2(x)\n",
    "    print(f\"Stage 2: Shape x: {x.shape}\")\n",
    "\n",
    "    # Stage 3\n",
    "    x = self.stage3(x, ch_out = False)\n",
    "    print(f\"Stage 3: Shape x: {x.shape}\")\n",
    "\n",
    "    # For classifiction; no decoder\n",
    "    if self.cls_token:\n",
    "      x = x[:, 1, :]\n",
    "      x = self.ff(x)\n",
    "    \n",
    "    # Perform decoding\n",
    "    else:\n",
    "      decoder = Decoder()\n",
    "      x = decoder(x)\n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering embedding: Patch Size: 7 -- Stride: 4 -- Embedding Dims 64\n",
      "Entering embedding: Patch Size: 3 -- Stride: 2 -- Embedding Dims 192\n",
      "Entering embedding: Patch Size: 3 -- Stride: 2 -- Embedding Dims 384\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  \n",
    "embed_size = 64\n",
    "num_class = 10\n",
    "model = CvT(embed_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X IN: torch.Size([16, 3, 224, 224])\n",
      "Shape X OUT: torch.Size([16, 3025, 64])\n",
      "Stage 1: Shape x: torch.Size([16, 64, 55, 55])\n",
      "Shape X IN: torch.Size([16, 64, 55, 55])\n",
      "Shape X OUT: torch.Size([16, 729, 192])\n",
      "Stage 2: Shape x: torch.Size([16, 192, 27, 27])\n",
      "Shape X IN: torch.Size([16, 192, 27, 27])\n",
      "Shape X OUT: torch.Size([16, 169, 384])\n",
      "Stage 3: Shape x: torch.Size([16, 384, 13, 13])\n",
      "After Stage 3 → 2: torch.Size([16, 192, 27, 27])\n",
      "After Stage 2 → 1: torch.Size([16, 64, 55, 55])\n",
      "After Stage 1 → Original: torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = model(single_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b31d6a8ecfd6b48ec0cf9da67896692b418c4a39f70d4f1a3880fdc530a94b5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
